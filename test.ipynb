{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Prerequisities......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import re\n",
    "import time\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from itertools import combinations\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from scipy import stats\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from scipy.stats import chi2_contingency,shapiro,spearmanr\n",
    "from summarytools import dfSummary\n",
    "import os\n",
    "# os.environ['GOOGLE_API_KEY']=os.eviron\n",
    "\n",
    "abstract_data = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'data.csv')\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Product Category</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Price per Unit</th>\n",
       "      <th>Total Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-24</td>\n",
       "      <td>CUST001</td>\n",
       "      <td>Male</td>\n",
       "      <td>34</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-27</td>\n",
       "      <td>CUST002</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>CUST003</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-05-21</td>\n",
       "      <td>CUST004</td>\n",
       "      <td>Male</td>\n",
       "      <td>37</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-05-06</td>\n",
       "      <td>CUST005</td>\n",
       "      <td>Male</td>\n",
       "      <td>30</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Transaction ID        Date Customer ID  Gender  Age Product Category  \\\n",
       "0               1  2023-11-24     CUST001    Male   34           Beauty   \n",
       "1               2  2023-02-27     CUST002  Female   26         Clothing   \n",
       "2               3  2023-01-13     CUST003    Male   50      Electronics   \n",
       "3               4  2023-05-21     CUST004    Male   37         Clothing   \n",
       "4               5  2023-05-06     CUST005    Male   30           Beauty   \n",
       "\n",
       "   Quantity  Price per Unit  Total Amount  \n",
       "0         3              50           150  \n",
       "1         2             500          1000  \n",
       "2         1              30            30  \n",
       "3         1             500           500  \n",
       "4         2              50           100  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_data['Top few samples of data'] = df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Type Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = {\n",
    "    'date': [r'\\d{4}-\\d{2}-\\d{2}', r'\\d{2}/\\d{2}/\\d{4}', r'\\d{2}-\\d{2}-\\d{4}'],\n",
    "    'percentage': [r'\\d+%'],\n",
    "    'int':[r'\\d+'],\n",
    "    'price': [r'\\$\\d+', r'\\d+\\.\\d{2}', r'\\d+,\\d+']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_dtype(patterns):\n",
    "    dtypes = df.dtypes.tolist()\n",
    "    cols = df.columns.tolist()\n",
    "    for i in range(len(cols)):\n",
    "        df[cols] = df[cols].replace('',np.nan)\n",
    "        if dtypes[i] == 'object':\n",
    "            sam = df[cols[i]].unique()[0]\n",
    "            cor,m='',0\n",
    "            for dtype,pats in patterns.items():\n",
    "                for pat in pats:\n",
    "                    l=len(re.findall(pat,str(sam)))\n",
    "                    # print(l)\n",
    "                    if l!=None and l>m:\n",
    "                        m=l\n",
    "                        cor=dtype\n",
    "            if cor == 'date':\n",
    "                df[cols[i]] = pd.to_datetime(df[cols[i]],errors='coerce')\n",
    "            elif cor == 'percentage':\n",
    "                df[cols[i]] = df[cols[i]].str.replace('%','').astype('float')\n",
    "            elif cor == 'int':\n",
    "                df[cols[i]] = pd.to_numeric(df[cols[i]],errors='coerce')\n",
    "            elif cor == 'price':\n",
    "                df[cols[i]] = df[cols[i]].str.replace('$','').replace(',','').astype('float')\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_dtype(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treating Nan Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_null(impute=False):\n",
    "    col_nulls = df.isnull().sum()\n",
    "    nulls_50 = []\n",
    "    impute_num = []\n",
    "    impute_cat = []\n",
    "    for col,null in col_nulls.items():\n",
    "        if null==0:\n",
    "            continue\n",
    "        if null>df.shape[0]*0.5:\n",
    "            nulls_50.append(col)\n",
    "            continue\n",
    "        elif impute:\n",
    "            if 'int' in str(df[col].dtypes):\n",
    "                impute_num.append(col)\n",
    "            else:\n",
    "                impute_cat.append(col)\n",
    "    df.drop(nulls_50,axis=1,inplace=True)\n",
    "    return nulls_50,impute_num,impute_cat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputer():\n",
    "    drop,im_num,im_cat=handle_null(True)\n",
    "    \n",
    "    rfr = RandomForestRegressor()\n",
    "    rfc = RandomForestClassifier()\n",
    "    \n",
    "    X = df[df.select_dtypes(include=['int','float']).columns]\n",
    "    imputer_num = IterativeImputer(estimator = rfr,max_iter=100)\n",
    "    imputer_num.fit(X)\n",
    "    df[df.select_dtypes(include=['int','float']).columns] = imputer_num.transform(X)\n",
    "    \n",
    "    for col in im_cat:\n",
    "        imputer_cat = IterativeImputer(estimator = rfc)\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(df[col])\n",
    "        X = df[df.select_dtypes(include=['int','float']).columns]\n",
    "        imputer_cat.fit(X,y=y)\n",
    "        y = imputer_cat.transform(X)\n",
    "        df[col] = le.inverse_transform(y.round().astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_null()\n",
    "imputer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns\n",
    "abstract_data['Columns present in data'] = columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About Transaction ID \n",
      "count    1000.000000\n",
      "mean      500.500000\n",
      "std       288.819436\n",
      "min         1.000000\n",
      "25%       250.750000\n",
      "50%       500.500000\n",
      "75%       750.250000\n",
      "max      1000.000000\n",
      "Name: Transaction ID, dtype: float64\n",
      "Unique values:1000\n",
      "Coefficient of Variation 0.5771\n",
      "About Gender \n",
      "count       1000\n",
      "unique         2\n",
      "top       Female\n",
      "freq         510\n",
      "Name: Gender, dtype: object\n",
      "Unique values:2\n",
      "About Age \n",
      "count    1000.00000\n",
      "mean       41.39200\n",
      "std        13.68143\n",
      "min        18.00000\n",
      "25%        29.00000\n",
      "50%        42.00000\n",
      "75%        53.00000\n",
      "max        64.00000\n",
      "Name: Age, dtype: float64\n",
      "Unique values:47\n",
      "Coefficient of Variation 0.3305\n",
      "About Product Category \n",
      "count         1000\n",
      "unique           3\n",
      "top       Clothing\n",
      "freq           351\n",
      "Name: Product Category, dtype: object\n",
      "Unique values:3\n",
      "About Quantity \n",
      "count    1000.000000\n",
      "mean        2.514000\n",
      "std         1.132734\n",
      "min         1.000000\n",
      "25%         1.000000\n",
      "50%         3.000000\n",
      "75%         4.000000\n",
      "max         4.000000\n",
      "Name: Quantity, dtype: float64\n",
      "Unique values:4\n",
      "Coefficient of Variation 0.4506\n",
      "About Price per Unit \n",
      "count    1000.000000\n",
      "mean      179.890000\n",
      "std       189.681356\n",
      "min        25.000000\n",
      "25%        30.000000\n",
      "50%        50.000000\n",
      "75%       300.000000\n",
      "max       500.000000\n",
      "Name: Price per Unit, dtype: float64\n",
      "Unique values:5\n",
      "Coefficient of Variation 1.0544\n",
      "About Total Amount \n",
      "count    1000.000000\n",
      "mean      456.000000\n",
      "std       559.997632\n",
      "min        25.000000\n",
      "25%        60.000000\n",
      "50%       135.000000\n",
      "75%       900.000000\n",
      "max      2000.000000\n",
      "Name: Total Amount, dtype: float64\n",
      "Unique values:18\n",
      "Coefficient of Variation 1.2281\n"
     ]
    }
   ],
   "source": [
    "cov = []\n",
    "col_des = []\n",
    "for column in columns:\n",
    "    print(f\"About {column} \\n{df[column].describe()}\\nUnique values:{df[column].nunique()}\")\n",
    "    col_des.append(f\"About {column} \\n{df[column].describe()}\\nUnique values:{df[column].nunique()}\")\n",
    "    try:\n",
    "        print(f\"Coefficient of Variation %.4f\"%(df[column].std()/df[column].mean()))\n",
    "        cov.append(f\"Coefficient of Variation of {column} is : %.4f\"%(df[column].std()/df[column].mean()))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_data['description about each and every columns'] = col_des\n",
    "abstract_data['coefficient of variations'] = cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns:\n",
    "    if df[column].nunique()>15:\n",
    "        df_summary.drop(column,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = dfSummary(df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_excel('summary.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_xlsx = pd.read_excel('summary.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>No</th>\n",
       "      <th>Variable</th>\n",
       "      <th>Stats / Values</th>\n",
       "      <th>Freqs / (% of Valid)</th>\n",
       "      <th>Graph</th>\n",
       "      <th>Missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;strong&gt;Gender&lt;/strong&gt;&lt;br&gt;[object]</td>\n",
       "      <td>1. Female&lt;br&gt;2. Male</td>\n",
       "      <td>510 (51.0%)&lt;br&gt;490 (49.0%)</td>\n",
       "      <td>&lt;img src = \"data:image/png;base64, iVBORw0KGgo...</td>\n",
       "      <td>0&lt;br&gt;(0.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;strong&gt;Product Category&lt;/strong&gt;&lt;br&gt;[object]</td>\n",
       "      <td>1. Clothing&lt;br&gt;2. Electronics&lt;br&gt;3. Beauty</td>\n",
       "      <td>351 (35.1%)&lt;br&gt;342 (34.2%)&lt;br&gt;307 (30.7%)</td>\n",
       "      <td>&lt;img src = \"data:image/png;base64, iVBORw0KGgo...</td>\n",
       "      <td>0&lt;br&gt;(0.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;strong&gt;Quantity&lt;/strong&gt;&lt;br&gt;[int64]</td>\n",
       "      <td>Mean (sd) : 2.5 (1.1)&lt;br&gt;min &lt; med &lt; max:&lt;br&gt;1...</td>\n",
       "      <td>4 distinct values</td>\n",
       "      <td>&lt;img src = \"data:image/png;base64, iVBORw0KGgo...</td>\n",
       "      <td>0&lt;br&gt;(0.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;strong&gt;Price per Unit&lt;/strong&gt;&lt;br&gt;[int64]</td>\n",
       "      <td>Mean (sd) : 179.9 (189.7)&lt;br&gt;min &lt; med &lt; max:&lt;...</td>\n",
       "      <td>5 distinct values</td>\n",
       "      <td>&lt;img src = \"data:image/png;base64, iVBORw0KGgo...</td>\n",
       "      <td>0&lt;br&gt;(0.0%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  No                                       Variable  \\\n",
       "0           0   1            <strong>Gender</strong><br>[object]   \n",
       "1           1   2  <strong>Product Category</strong><br>[object]   \n",
       "2           2   3           <strong>Quantity</strong><br>[int64]   \n",
       "3           3   4     <strong>Price per Unit</strong><br>[int64]   \n",
       "\n",
       "                                      Stats / Values  \\\n",
       "0                               1. Female<br>2. Male   \n",
       "1         1. Clothing<br>2. Electronics<br>3. Beauty   \n",
       "2  Mean (sd) : 2.5 (1.1)<br>min < med < max:<br>1...   \n",
       "3  Mean (sd) : 179.9 (189.7)<br>min < med < max:<...   \n",
       "\n",
       "                        Freqs / (% of Valid)  \\\n",
       "0                 510 (51.0%)<br>490 (49.0%)   \n",
       "1  351 (35.1%)<br>342 (34.2%)<br>307 (30.7%)   \n",
       "2                          4 distinct values   \n",
       "3                          5 distinct values   \n",
       "\n",
       "                                               Graph      Missing  \n",
       "0  <img src = \"data:image/png;base64, iVBORw0KGgo...  0<br>(0.0%)  \n",
       "1  <img src = \"data:image/png;base64, iVBORw0KGgo...  0<br>(0.0%)  \n",
       "2  <img src = \"data:image/png;base64, iVBORw0KGgo...  0<br>(0.0%)  \n",
       "3  <img src = \"data:image/png;base64, iVBORw0KGgo...  0<br>(0.0%)  "
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f3806 thead>tr>th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_f3806_row0_col0, #T_f3806_row1_col0, #T_f3806_row2_col0, #T_f3806_row3_col0 {\n",
       "  text-align: left;\n",
       "  font-size: 12px;\n",
       "  vertical-align: middle;\n",
       "  width: 5%;\n",
       "  max-width: 50px;\n",
       "  min-width: 20px;\n",
       "}\n",
       "#T_f3806_row0_col1, #T_f3806_row1_col1, #T_f3806_row2_col1, #T_f3806_row3_col1 {\n",
       "  text-align: left;\n",
       "  font-size: 12px;\n",
       "  vertical-align: middle;\n",
       "  width: 15%;\n",
       "  max-width: 200px;\n",
       "  min-width: 100px;\n",
       "  word-break: break-word;\n",
       "}\n",
       "#T_f3806_row0_col2, #T_f3806_row1_col2, #T_f3806_row2_col2, #T_f3806_row3_col2 {\n",
       "  text-align: left;\n",
       "  font-size: 12px;\n",
       "  vertical-align: middle;\n",
       "  width: 30%;\n",
       "  min-width: 100px;\n",
       "}\n",
       "#T_f3806_row0_col3, #T_f3806_row1_col3, #T_f3806_row2_col3, #T_f3806_row3_col3 {\n",
       "  text-align: left;\n",
       "  font-size: 12px;\n",
       "  vertical-align: middle;\n",
       "  width: 25%;\n",
       "  min-width: 100px;\n",
       "}\n",
       "#T_f3806_row0_col4, #T_f3806_row1_col4, #T_f3806_row2_col4, #T_f3806_row3_col4 {\n",
       "  text-align: left;\n",
       "  font-size: 12px;\n",
       "  vertical-align: middle;\n",
       "  width: 20%;\n",
       "  min-width: 150px;\n",
       "}\n",
       "#T_f3806_row0_col5, #T_f3806_row1_col5, #T_f3806_row2_col5, #T_f3806_row3_col5 {\n",
       "  text-align: left;\n",
       "  font-size: 12px;\n",
       "  vertical-align: middle;\n",
       "  width: 10%;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f3806\">\n",
       "  <caption><strong>Data Frame Summary</strong><br>df_summary<br>Dimensions: 1,000 x 4<br>Duplicates: 880</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_f3806_level0_col0\" class=\"col_heading level0 col0\" >No</th>\n",
       "      <th id=\"T_f3806_level0_col1\" class=\"col_heading level0 col1\" >Variable</th>\n",
       "      <th id=\"T_f3806_level0_col2\" class=\"col_heading level0 col2\" >Stats / Values</th>\n",
       "      <th id=\"T_f3806_level0_col3\" class=\"col_heading level0 col3\" >Freqs / (% of Valid)</th>\n",
       "      <th id=\"T_f3806_level0_col4\" class=\"col_heading level0 col4\" >Graph</th>\n",
       "      <th id=\"T_f3806_level0_col5\" class=\"col_heading level0 col5\" >Missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_f3806_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_f3806_row0_col1\" class=\"data row0 col1\" ><strong>Gender</strong><br>[object]</td>\n",
       "      <td id=\"T_f3806_row0_col2\" class=\"data row0 col2\" >1. Female<br>2. Male</td>\n",
       "      <td id=\"T_f3806_row0_col3\" class=\"data row0 col3\" >510 (51.0%)<br>490 (49.0%)</td>\n",
       "      <td id=\"T_f3806_row0_col4\" class=\"data row0 col4\" ><img src = \"data:image/png;base64, iVBORw0KGgoAAAANSUhEUgAAAJsAAAAuCAYAAAA/ZmtKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABOUlEQVR4nO3bQWrDMBRF0a8iQjORMAYvxovoYrsI7cYYa9CETNRBN5BvwxOi96zgYy6GBL/QWjNAIZjZ3cxuvQ8Z1Ku19uh9xCjiPM9fKaWp9yEjqrXuIYRvgntPTClN67o+cs7P3seM5DiOz1LKtG3bzcyI7Q3RzCzn/FyW5af3MQO69z5gJB+9D8D/QWyQITbIEBtkiA0yxAYZYoMMsUGG2CBDbJAhNsgQG2SIDTLEBhlig0w0+/sQsPcho+GZ+cVa615KmYwPAd1qrbuZvXrfMQoGL9cweHEITPmgwpvNj7fZSUz5nJjvnceUz4H53jVM+fz41X4Sf+pChtggQ2yQITbIEBtkiA0yxAYZYoMMsUGG2CBDbJAhNsgQG2SIDTLEBhmmfA48p2uY8jkx3zuPwYsfg5eTfgEKW2rfqvpg/AAAAABJRU5ErkJggg==\"></img></td>\n",
       "      <td id=\"T_f3806_row0_col5\" class=\"data row0 col5\" >0<br>(0.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f3806_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_f3806_row1_col1\" class=\"data row1 col1\" ><strong>Product Category</strong><br>[object]</td>\n",
       "      <td id=\"T_f3806_row1_col2\" class=\"data row1 col2\" >1. Clothing<br>2. Electronics<br>3. Beauty</td>\n",
       "      <td id=\"T_f3806_row1_col3\" class=\"data row1 col3\" >351 (35.1%)<br>342 (34.2%)<br>307 (30.7%)</td>\n",
       "      <td id=\"T_f3806_row1_col4\" class=\"data row1 col4\" ><img src = \"data:image/png;base64, iVBORw0KGgoAAAANSUhEUgAAAJsAAABFCAYAAABdVZTTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABlElEQVR4nO3bQWrCUBRG4ZsSpE4SguBiXEQX20VkNyK+gYqTdNCBdJ6el3c93wqucFAS/LtlWUIifNQ+QO+ji4h9ROxqH/IPnsuy3GsfoZf+cDh8DcMw1T5kbaWUS9d13wa3Hf0wDNPpdLqP4/iofcxartfr5zzP0/l83kWEsW1EHxExjuPjeDzeah+zsn3tA/SXDwjCGJswxiaMsQljbMIYmzDGJoyxCWNswhibMMYmjLEJY2zCGJswfcTv/79qH7KmbJ8ni76UcpnneYpk//8qpVwi4ln7Dr24QRCmc8onStZvNr/VNijluspl1TalW1e5rNqurOuqVE/WWfhSVxhjE8bYhDE2YYxNGGMTxtiEMTZhjE0YYxPG2IQxNmGMTRhjEybduirTZ8km5brKZdU2uUEQxnWVMD4gCJPhZ9SfzEY0P+VztteOpqd8zvbakmHKl+aVTXY+IAhjbMIYmzDGJoyxCWNswhibMMYmjLEJY2zCGJswxiaMsQljbMI0PeVr9e531fyUz9leO9wgCOOUT5gfSryjSvQoxPgAAAAASUVORK5CYII=\"></img></td>\n",
       "      <td id=\"T_f3806_row1_col5\" class=\"data row1 col5\" >0<br>(0.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f3806_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "      <td id=\"T_f3806_row2_col1\" class=\"data row2 col1\" ><strong>Quantity</strong><br>[int64]</td>\n",
       "      <td id=\"T_f3806_row2_col2\" class=\"data row2 col2\" >Mean (sd) : 2.5 (1.1)<br>min < med < max:<br>1.0 < 3.0 < 4.0<br>IQR (CV) : 3.0 (2.2)</td>\n",
       "      <td id=\"T_f3806_row2_col3\" class=\"data row2 col3\" >4 distinct values</td>\n",
       "      <td id=\"T_f3806_row2_col4\" class=\"data row2 col4\" ><img src = \"data:image/png;base64, iVBORw0KGgoAAAANSUhEUgAAAKoAAABGCAYAAABc8A97AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABsklEQVR4nO3ZQWrCQBhA4X9KEN1MEMHDeIgetofwNlLGRZWCpheoNiZO4wvv28aRWbyZP2Lqui6kV/c29QakPpqpN6D5SCmtImIxcPl313WnWw8NVU+RUlptNpv3nPN6yPrj8fiZUvq4Fauh6lkWOef1brc7tW17fmRhKWW53+/Xh8NhERHDQq15nWt+2rY9b7fbrwFLV/ce3g219nUu9fXXjVr1Oq9p5CSIcBq8lF7vqLWu81rGToKI6aaBB+x3c/0xNXgSREw3DcgHrLa5hhoRoyZBxDTTAHnA/sOsQ6UCHrDq/AtVCIYqBEMVgqEKwVCFYKhCMFQhGKoQDFUIhioEQxWCoQrBUIVgqEIwVCEYqhAMVQiGKgRDFYKhCsFQhWCoQjBUIRiqEAxVCIYqBEMVgqEKwVCFYKhCMFQhGKoQDFUIhioEQxWCoQrBUIVgqEIwVCEYqhAMVQiGKgRDFYKhCsFQhWCoQjBUIRiqEAxVCIYqBEMVgqEKwVCFYKhCMFQhGKoQDFUITZ8PlVKWj35xKWV5uVyaiMgppYc3NlK+Xq/NkH1HTLp36r4jRuy9z5ofVkSfe2QF2BQAAAAASUVORK5CYII=\"></img></td>\n",
       "      <td id=\"T_f3806_row2_col5\" class=\"data row2 col5\" >0<br>(0.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f3806_row3_col0\" class=\"data row3 col0\" >4</td>\n",
       "      <td id=\"T_f3806_row3_col1\" class=\"data row3 col1\" ><strong>Price per Unit</strong><br>[int64]</td>\n",
       "      <td id=\"T_f3806_row3_col2\" class=\"data row3 col2\" >Mean (sd) : 179.9 (189.7)<br>min < med < max:<br>25.0 < 50.0 < 500.0<br>IQR (CV) : 270.0 (0.9)</td>\n",
       "      <td id=\"T_f3806_row3_col3\" class=\"data row3 col3\" >5 distinct values</td>\n",
       "      <td id=\"T_f3806_row3_col4\" class=\"data row3 col4\" ><img src = \"data:image/png;base64, iVBORw0KGgoAAAANSUhEUgAAAKoAAABGCAYAAABc8A97AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABj0lEQVR4nO3dQYriUBSG0fcaEZ0kiOBiXEQvthfhbqR5Dkpp0PSsJwWlRkPXT84ZJ5dc+VAykFeHYSjw3f343w8Aj1jcu6DWui6lLEfO/zMMw3nkvfDPl6HWWtfb7fZn13WbMcNPp9PvWusvsfKqe9+oy67rNvv9/tz3/eWZwa211eFw2ByPx2UpRai85O5Pfyml9H1/2e12HyPmr0fcA594mSKCUIkgVCIIlQhCJYJQiSBUIgiVCEIlglCJIFQiCJUIQiWCUIkgVCIIlQhCJYJQiSBUIgiVCEIlglCJIFQiCJUIQiWCUIkgVCIIlQhCJYJQiSBUIgiVCEIlglCJIFQiCJUIQiWCUIkgVCIIlQhCJcJDJ/fxnLkedDzl3kJ9s7kedDz13kJ9v7kedDzp3kKdyFwPOp5qby9TRBAqEYRKBKESQahEECoRhEoEoRJBqEQQKhGESgShEkGoRBAqEYRKBKESQahEECoRHvorSmtt9ezg1trqer0uSildrfXpBwvW3W63xQw/s5f2vnfNX7BYdvroihxBAAAAAElFTkSuQmCC\"></img></td>\n",
       "      <td id=\"T_f3806_row3_col5\" class=\"data row3 col5\" >0<br>(0.0%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x27ef960e190>"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_data['Summary of the data'] = sum_xlsx\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Top few samples of data':    Transaction ID  Gender   Age Product Category  Quantity  Price per Unit  \\\n",
       " 0             1.0    Male  34.0           Beauty       3.0            50.0   \n",
       " 1             2.0  Female  26.0         Clothing       2.0           500.0   \n",
       " 2             3.0    Male  50.0      Electronics       1.0            30.0   \n",
       " 3             4.0    Male  37.0         Clothing       1.0           500.0   \n",
       " 4             5.0    Male  30.0           Beauty       2.0            50.0   \n",
       " \n",
       "    Total Amount  \n",
       " 0         150.0  \n",
       " 1        1000.0  \n",
       " 2          30.0  \n",
       " 3         500.0  \n",
       " 4         100.0  ,\n",
       " 'Columns present in data': ['Transaction ID',\n",
       "  'Gender',\n",
       "  'Age',\n",
       "  'Product Category',\n",
       "  'Quantity',\n",
       "  'Price per Unit',\n",
       "  'Total Amount'],\n",
       " 'description about each and every columns': ['About Transaction ID \\ncount    1000.000000\\nmean      500.500000\\nstd       288.819436\\nmin         1.000000\\n25%       250.750000\\n50%       500.500000\\n75%       750.250000\\nmax      1000.000000\\nName: Transaction ID, dtype: float64\\nUnique values:1000',\n",
       "  'About Gender \\ncount       1000\\nunique         2\\ntop       Female\\nfreq         510\\nName: Gender, dtype: object\\nUnique values:2',\n",
       "  'About Age \\ncount    1000.00000\\nmean       41.39200\\nstd        13.68143\\nmin        18.00000\\n25%        29.00000\\n50%        42.00000\\n75%        53.00000\\nmax        64.00000\\nName: Age, dtype: float64\\nUnique values:47',\n",
       "  'About Product Category \\ncount         1000\\nunique           3\\ntop       Clothing\\nfreq           351\\nName: Product Category, dtype: object\\nUnique values:3',\n",
       "  'About Quantity \\ncount    1000.000000\\nmean        2.514000\\nstd         1.132734\\nmin         1.000000\\n25%         1.000000\\n50%         3.000000\\n75%         4.000000\\nmax         4.000000\\nName: Quantity, dtype: float64\\nUnique values:4',\n",
       "  'About Price per Unit \\ncount    1000.000000\\nmean      179.890000\\nstd       189.681356\\nmin        25.000000\\n25%        30.000000\\n50%        50.000000\\n75%       300.000000\\nmax       500.000000\\nName: Price per Unit, dtype: float64\\nUnique values:5',\n",
       "  'About Total Amount \\ncount    1000.000000\\nmean      456.000000\\nstd       559.997632\\nmin        25.000000\\n25%        60.000000\\n50%       135.000000\\n75%       900.000000\\nmax      2000.000000\\nName: Total Amount, dtype: float64\\nUnique values:18'],\n",
       " 'coefficient of variations': ['Coefficient of Variation of Transaction ID is : 0.5771',\n",
       "  'Coefficient of Variation of Age is : 0.3305',\n",
       "  'Coefficient of Variation of Quantity is : 0.4506',\n",
       "  'Coefficient of Variation of Price per Unit is : 1.0544',\n",
       "  'Coefficient of Variation of Total Amount is : 1.2281'],\n",
       " 'Summary of the data':    Unnamed: 0  No                                       Variable  \\\n",
       " 0           0   1            <strong>Gender</strong><br>[object]   \n",
       " 1           1   2  <strong>Product Category</strong><br>[object]   \n",
       " 2           2   3           <strong>Quantity</strong><br>[int64]   \n",
       " 3           3   4     <strong>Price per Unit</strong><br>[int64]   \n",
       " \n",
       "                                       Stats / Values  \\\n",
       " 0                               1. Female<br>2. Male   \n",
       " 1         1. Clothing<br>2. Electronics<br>3. Beauty   \n",
       " 2  Mean (sd) : 2.5 (1.1)<br>min < med < max:<br>1...   \n",
       " 3  Mean (sd) : 179.9 (189.7)<br>min < med < max:<...   \n",
       " \n",
       "                         Freqs / (% of Valid)  \\\n",
       " 0                 510 (51.0%)<br>490 (49.0%)   \n",
       " 1  351 (35.1%)<br>342 (34.2%)<br>307 (30.7%)   \n",
       " 2                          4 distinct values   \n",
       " 3                          5 distinct values   \n",
       " \n",
       "                                                Graph      Missing  \n",
       " 0  <img src = \"data:image/png;base64, iVBORw0KGgo...  0<br>(0.0%)  \n",
       " 1  <img src = \"data:image/png;base64, iVBORw0KGgo...  0<br>(0.0%)  \n",
       " 2  <img src = \"data:image/png;base64, iVBORw0KGgo...  0<br>(0.0%)  \n",
       " 3  <img src = \"data:image/png;base64, iVBORw0KGgo...  0<br>(0.0%)  }"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Descriptive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-Square Test [CAT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes(include=['int','float']).columns.tolist()\n",
    "cat_cols = df.select_dtypes(exclude=['int','float']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_test(A,B):\n",
    "    contingency_table = pd.crosstab(A,B)\n",
    "    return  chi2_contingency(contingency_table)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to reject H0 due to lack of evidence under significance level 0.05 Gender & Product Category are independent.\n"
     ]
    }
   ],
   "source": [
    "ALPHA = 0.05\n",
    "chi = []\n",
    "for prev_col in range(len(cat_cols)):\n",
    "    for cur_col in range(prev_col+1,len(cat_cols)):\n",
    "        chi2_stat, p_val, dof, ex = chi_square_test(df[cat_cols[prev_col]],df[cat_cols[cur_col]])\n",
    "        res = f\"\"\" feature 1:{cat_cols[prev_col]} and feature 2: {cat_cols[cur_col]}\n",
    "        Chi-Square Statistic: {chi2_stat}\n",
    "        p-value: {p_val}\n",
    "        Degrees of freedom:{dof}\n",
    "        Expected frequencies:{ex}\"\"\"\n",
    "        if p_val <= ALPHA:\n",
    "            print(f'Rejected H0 under significance level {ALPHA} {cat_cols[prev_col]} & {cat_cols[cur_col]} are dependent.')\n",
    "            res+=f'\\nRejected H0 under significance level {ALPHA} {cat_cols[prev_col]} & {cat_cols[cur_col]} are dependent.'\n",
    "        else:\n",
    "            print(f'Fail to reject H0 due to lack of evidence under significance level {ALPHA} {cat_cols[prev_col]} & {cat_cols[cur_col]} are independent.')\n",
    "            res+=f'\\nFail to reject H0 due to lack of evidence under significance level {ALPHA} {cat_cols[prev_col]} & {cat_cols[cur_col]} are independent.'\n",
    "        chi.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_data['chi-square contigency test for features'] = chi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapiro-Wilkinson's Normality Test [NUM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rejected H0 under significance level 0.05\n",
      "Transaction ID doesn't seems to be normally distributed\n",
      "\n",
      "Rejected H0 under significance level 0.05\n",
      "Age doesn't seems to be normally distributed\n",
      "\n",
      "Rejected H0 under significance level 0.05\n",
      "Quantity doesn't seems to be normally distributed\n",
      "\n",
      "Rejected H0 under significance level 0.05\n",
      "Price per Unit doesn't seems to be normally distributed\n",
      "\n",
      "Rejected H0 under significance level 0.05\n",
      "Total Amount doesn't seems to be normally distributed\n"
     ]
    }
   ],
   "source": [
    "ALPHA = 0.05\n",
    "shap = ['Shapiro wilkinson normality test']\n",
    "for column in num_cols:\n",
    "    _,p = shapiro(df[column])\n",
    "    if p <= ALPHA:\n",
    "        print(f'''\\nRejected H0 under significance level {ALPHA}\\n{column} doesn't seems to be normally distributed''')\n",
    "        shap.append(f'''\\nRejected H0 under significance level {ALPHA}\\n{column} doesn't seems to be normally distributed''')\n",
    "    else:\n",
    "        print(f'''\\nFail to reject H0 due to lack of evidence under significance level {ALPHA}\\n{column} seem to be normally distributed''')\n",
    "        shap.append(f'''\\nFail to reject H0 due to lack of evidence under significance level {ALPHA}\\n{column} seem to be normally distributed''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_data['Shapiro wilkinson normality test'] = shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spearemann's correlation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* `Transaction ID` & `Age`\n",
      "\n",
      "corr: 0.0653 \t p: 0.03892114436932943\n",
      "Rejected H0 under significance level 0.05, Transaction ID & Age are correlated\n",
      "\n",
      "* `Transaction ID` & `Quantity`\n",
      "\n",
      "corr: -0.0268 \t p: 0.3973114959672085\n",
      "Fail to reject H0 due to lack of evidence under significance level 0.05, \n",
      "                Transaction ID & Quantity are not correlated\n",
      "\n",
      "* `Transaction ID` & `Price per Unit`\n",
      "\n",
      "corr: -0.0671 \t p: 0.03383808628700808\n",
      "Rejected H0 under significance level 0.05, Transaction ID & Price per Unit are correlated\n",
      "\n",
      "* `Transaction ID` & `Total Amount`\n",
      "\n",
      "corr: -0.0693 \t p: 0.028354546112173036\n",
      "Rejected H0 under significance level 0.05, Transaction ID & Total Amount are correlated\n",
      "\n",
      "* `Age` & `Quantity`\n",
      "\n",
      "corr: -0.0235 \t p: 0.45844430718442974\n",
      "Fail to reject H0 due to lack of evidence under significance level 0.05, \n",
      "                Age & Quantity are not correlated\n",
      "\n",
      "* `Age` & `Price per Unit`\n",
      "\n",
      "corr: -0.038 \t p: 0.23032617307313\n",
      "Fail to reject H0 due to lack of evidence under significance level 0.05, \n",
      "                Age & Price per Unit are not correlated\n",
      "\n",
      "* `Age` & `Total Amount`\n",
      "\n",
      "corr: -0.0379 \t p: 0.23158062202614216\n",
      "Fail to reject H0 due to lack of evidence under significance level 0.05, \n",
      "                Age & Total Amount are not correlated\n",
      "\n",
      "* `Quantity` & `Price per Unit`\n",
      "\n",
      "corr: 0.024 \t p: 0.44812729780729177\n",
      "Fail to reject H0 due to lack of evidence under significance level 0.05, \n",
      "                Quantity & Price per Unit are not correlated\n",
      "\n",
      "* `Quantity` & `Total Amount`\n",
      "\n",
      "corr: 0.4741 \t p: 3.545564749358996e-57\n",
      "Rejected H0 under significance level 0.05, Quantity & Total Amount are correlated\n",
      "\n",
      "* `Price per Unit` & `Total Amount`\n",
      "\n",
      "corr: 0.8574 \t p: 3.19403253106179e-290\n",
      "Rejected H0 under significance level 0.05, Price per Unit & Total Amount are correlated\n"
     ]
    }
   ],
   "source": [
    "corr = []\n",
    "for prev_col in range(len(num_cols)):\n",
    "    for cur_col in range(prev_col+1,len(num_cols)):\n",
    "        corr, p = spearmanr(df[num_cols[prev_col]], df[num_cols[cur_col]])\n",
    "        R = ''\n",
    "        print(f'\\n* `{num_cols[prev_col]}` & `{num_cols[cur_col]}`\\n')\n",
    "        R+=f'\\n* `{num_cols[prev_col]}` & `{num_cols[cur_col]}`\\n'\n",
    "        print(f'corr: {round(corr, 4)} \\t p: {p}')\n",
    "        R+=f'corr: {round(corr, 4)} \\t p: {p}'\n",
    "\n",
    "        if p <= ALPHA:\n",
    "            print(f'Rejected H0 under significance level {ALPHA}, {num_cols[prev_col]} & {num_cols[cur_col]} are correlated')\n",
    "            R+=f'Rejected H0 under significance level {ALPHA}, {num_cols[prev_col]} & {num_cols[cur_col]} are correlated'\n",
    "        else:\n",
    "            print(f'''Fail to reject H0 due to lack of evidence under significance level {ALPHA}, \n",
    "                {num_cols[prev_col]} & {num_cols[cur_col]} are not correlated''')\n",
    "            R+=f'''Fail to reject H0 due to lack of evidence under significance level {ALPHA}, \n",
    "                {num_cols[prev_col]} & {num_cols[cur_col]} are not correlated'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_data['Correaltions information about features'] = corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_cat_cols = []\n",
    "aggregations = dict()\n",
    "for col in cat_cols:\n",
    "        aggregations[\"aggregations  & grouping by \"+col] = df.groupby(col)[cat_cols].agg({'max','min',pd.Series.mode})\n",
    "        aggregations[\"aggregations & grouping by  \"+col] = df.groupby(col)[num_cols].agg({'max','count',pd.Series.mean})\n",
    "        diff_cat_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(diff_cat_cols)-1):\n",
    "    for j in range(i+1,len(diff_cat_cols)):\n",
    "        grp_data = df.groupby([diff_cat_cols[i],diff_cat_cols[j]]).size().unstack().fillna(0)\n",
    "        aggregations['data_aggregations'+diff_cat_cols[i]+diff_cat_cols[j]] = grp_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_data['Grouping among features'] = aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anomolies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = pd.get_dummies(df,dtype='float')\n",
    "data_num.dropna(inplace=True)\n",
    "mean_vector = np.mean(data_num, axis=0)  \n",
    "cov_matrix = np.cov(data_num, rowvar=False) \n",
    "\n",
    "inv_cov_matrix = np.linalg.inv(cov_matrix+0.000001) \n",
    "\n",
    "def mahalanobis(x):\n",
    "    centered_x = x - mean_vector  # (11,)\n",
    "    mahalanobis_dist = np.dot(centered_x, np.dot(inv_cov_matrix, centered_x.T))  # (1,)\n",
    "    return mahalanobis_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num['mahalanobis'] = data_num.apply(lambda row: mahalanobis(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data = df.copy()\n",
    "out_data.dropna(inplace=True)\n",
    "out_data['outlier'] = [1 if i>(data_num['mahalanobis'].mean())*3  else 0 for i in data_num['mahalanobis']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = out_data[out_data['outlier']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_data['Outliers points present in data'] = outliers.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inertia = []\n",
    "# si_score = []\n",
    "# wc_score = []\n",
    "# for i in range(2,11):\n",
    "#     km = KMeans(n_clusters=i)\n",
    "#     km.fit(data[['Annual Income (k$)','Spending Score (1-100)']]) \n",
    "#     inertia.append(km.inertia_)\n",
    "#     si_score.append(silhouette_score(data[['Annual Income (k$)','Spending Score (1-100)']],km.labels_))\n",
    "#     wc_score.append(davies_bouldin_score(data[['Annual Income (k$)','Spending Score (1-100)']],km.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "LE = dict()\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    cluster_df[col] = le.fit_transform(cluster_df[col])\n",
    "    LE[col]=le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Number of Clusters: 5\n",
      "Best Feature Set: ['Gender', 'Price per Unit', 'Total Amount'] with Silhouette Score: 0.7255583888772646\n",
      "Best labels: [0 1 0 4 0 2 0 0 4 2 0 0 3 0 1 3 2 2 2 4 1 0 2 2 2 1 2 1 2 1 4 0 2 2 1 4 2\n",
      " 0 0 0 0 4 2 2 2 1 1 4 1 2 0 2 0 1 0 1 2 4 0 0 0 0 0 0 3 2 1 4 2 2 2 1 0 1\n",
      " 0 2 2 1 4 2 0 2 0 2 0 0 2 4 1 2 1 2 1 1 2 1 1 2 1 0 4 2 2 1 1 2 1 2 1 4 1\n",
      " 3 2 2 3 2 3 1 2 0 2 0 2 3 0 2 2 4 1 1 1 0 4 0 0 4 3 0 3 0 2 4 2 1 2 0 4 0\n",
      " 0 2 0 3 3 4 3 2 3 1 0 2 3 0 2 1 1 3 2 4 3 2 1 0 0 2 2 2 0 0 4 4 4 0 1 0 0\n",
      " 0 2 0 0 2 0 0 3 0 0 1 2 1 3 0 0 1 3 0 2 0 2 2 2 0 3 3 3 0 3 0 2 0 2 4 4 0\n",
      " 2 2 2 2 0 2 0 0 2 2 1 2 1 2 1 1 3 2 2 0 1 0 0 2 0 4 0 0 2 4 1 4 0 3 3 2 2\n",
      " 0 0 2 0 4 4 2 2 2 3 4 2 2 2 1 3 2 0 2 4 1 1 2 1 0 2 0 0 0 0 1 4 4 0 2 1 1\n",
      " 1 4 3 2 0 4 0 2 2 0 2 1 2 2 2 0 1 0 0 2 0 0 4 1 2 4 1 2 2 2 0 0 2 2 0 4 1\n",
      " 4 2 2 4 0 2 1 0 1 0 2 0 3 0 1 2 0 2 3 4 2 1 3 2 2 0 0 1 0 0 1 4 0 2 1 3 0\n",
      " 2 1 1 2 0 2 2 4 2 4 2 1 2 4 3 1 0 0 0 0 0 4 1 1 3 2 2 1 2 0 2 1 4 3 1 2 1\n",
      " 1 1 2 0 1 2 0 0 3 4 1 2 1 1 2 2 4 2 0 0 2 0 1 4 1 0 2 1 2 1 2 0 4 4 2 4 2\n",
      " 2 0 3 2 0 2 2 1 1 2 0 0 1 2 4 0 1 4 1 4 2 0 2 0 0 1 0 1 0 1 0 1 0 2 4 1 1\n",
      " 1 0 1 0 2 3 1 0 0 1 0 0 2 0 4 0 2 0 2 0 0 3 2 0 3 1 4 1 2 0 2 0 2 1 0 2 2\n",
      " 2 2 2 3 2 4 2 0 0 2 2 2 4 2 3 3 0 2 1 0 4 1 4 2 4 2 0 2 3 2 2 4 4 2 4 2 4\n",
      " 2 2 2 1 2 1 0 0 0 2 2 1 2 0 4 2 3 0 2 0 2 3 2 2 1 2 4 2 2 2 0 1 0 1 4 0 1\n",
      " 0 1 1 2 4 0 2 1 0 2 2 2 3 0 0 1 2 1 3 1 2 1 2 0 0 2 0 0 1 2 0 1 4 1 0 2 0\n",
      " 0 0 2 0 3 1 1 4 4 2 2 2 2 2 0 2 0 3 4 1 0 0 2 0 0 1 0 0 0 2 1 2 3 4 1 0 0\n",
      " 1 2 4 0 0 2 1 2 2 3 1 1 2 1 2 4 3 1 0 2 2 0 0 1 2 2 3 2 2 2 4 2 2 3 2 1 0\n",
      " 2 0 0 1 1 1 1 3 2 0 1 2 1 4 2 2 1 1 4 2 0 4 4 4 0 4 2 3 3 0 2 1 0 2 0 0 2\n",
      " 4 1 1 0 0 2 0 0 0 2 2 0 2 2 2 1 1 0 0 4 1 2 0 2 0 4 0 2 2 0 0 0 3 2 2 0 0\n",
      " 2 1 0 4 4 2 1 2 0 0 1 1 0 2 2 0 2 4 0 0 0 0 4 0 2 0 0 1 1 2 3 2 0 0 0 0 1\n",
      " 2 3 0 4 2 0 4 2 2 0 2 2 4 1 0 2 0 3 0 2 0 2 0 4 1 0 0 1 3 0 4 0 1 2 0 1 0\n",
      " 2 3 0 0 0 0 0 1 0 2 4 2 1 2 0 4 2 3 2 0 2 2 0 1 0 2 2 0 3 4 2 4 2 2 4 0 2\n",
      " 2 0 4 0 0 0 2 2 2 2 4 0 0 2 2 4 4 2 2 4 4 2 4 0 0 1 2 2 2 2 2 2 0 0 4 0 4\n",
      " 0 3 1 2 0 0 2 0 4 2 0 1 0 2 2 2 3 1 0 0 3 4 2 2 4 0 2 0 1 0 3 2 0 2 0 0 0\n",
      " 2 4 0 3 0 1 1 3 2 0 0 0 2 1 2 2 2 2 2 2 2 4 2 1 1 2 2 1 2 2 2 1 2 0 0 2 2\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def select_best_features_and_clusters(df, min_clusters=2, max_clusters=10, top_n=5):\n",
    "    # Scale the data only once\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    labels = None\n",
    "    best_score = -1\n",
    "    best_features = None\n",
    "    best_n_clusters = None\n",
    "    column_names = df.columns\n",
    "\n",
    "    # Iterate over different numbers of clusters\n",
    "    for n_clusters in range(min_clusters, max_clusters + 1):\n",
    "        # Iterate over all combinations of features\n",
    "        for n_features in range(1, top_n + 1):\n",
    "            for feature_subset in combinations(range(df.shape[1]), n_features):\n",
    "                if len(feature_subset)<=2:\n",
    "                    continue\n",
    "                # Select feature subset from scaled data\n",
    "                subset_data = scaled_data[:, feature_subset]\n",
    "\n",
    "                # Apply KMeans clustering\n",
    "                kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "                cluster_labels = kmeans.fit_predict(subset_data)\n",
    "\n",
    "                # Compute silhouette score\n",
    "                score = silhouette_score(subset_data, cluster_labels)\n",
    "                \n",
    "                # Keep track of the best score, features, and number of clusters\n",
    "                if score > best_score :\n",
    "                    best_score = score\n",
    "                    labels = cluster_labels\n",
    "                    best_features = feature_subset\n",
    "                    best_n_clusters = n_clusters\n",
    "\n",
    "    # Get the selected feature names\n",
    "    selected_feature_names = [column_names[i] for i in best_features]\n",
    "    \n",
    "    print(f\"Best Number of Clusters: {best_n_clusters}\")\n",
    "    print(f\"Best Feature Set: {selected_feature_names} with Silhouette Score: {best_score}\")\n",
    "    # print(f\"Best labels: {labels}\")\n",
    "    # Return the DataFrame with selected features and the best number of clusters\n",
    "    return df[selected_feature_names], best_n_clusters,labels\n",
    "\n",
    "# Example usage\n",
    "# best_features_silhouette, best_n_clusters,labels = select_best_features_and_clusters(cluster_df, min_clusters=2, max_clusters=5, top_n=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_df['label']=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract_data['clusters'] = cluster_df.groupby('label')[cat_cols].aggregate(pd.Series.mode).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract_data['interpretaions among cluster segments'] = cluster_df.groupby('label')[num_cols].aggregate('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_executor(prompt):\n",
    "    head = df.head(6)\n",
    "    model =  ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-pro\", \n",
    "        safety_settings=None\n",
    "    )\n",
    "    result = 'Error'\n",
    "    while 'Error' in result:\n",
    "        response = model.invoke(prompt)\n",
    "        # print(response.text)    \n",
    "        time.sleep(3)\n",
    "        print(response)\n",
    "        res = response.content\n",
    "        right = res.rfind('```')\n",
    "        left = res.find('```')\n",
    "        code = res[left+10:right]\n",
    "        repl = PythonREPL()\n",
    "        result = repl.run(code)\n",
    "        # print(code)\n",
    "        print(result)\n",
    "        prompt=f\"by running your code {code} please correct the code please solve the error : {result} for   \"+prompt\n",
    "       \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = df.head()\n",
    "prompt = f\"\"\"Generate a Python code for Time series forecasting for  the retail data[data.csv] by pmd autoarima forecast next month sales and save the results has columns = {columns} and some sample of the data {head} \n",
    "if the data is not suitable for time series analysis such that if it doesn't has data column do some other forecasting without date - if it is not suitable for forescasting just print ( False or 0)\n",
    "save the plot in (time_series.png)\n",
    "and forecast data in (forecast.csv) \n",
    "dont use exit() keyword instead use if else\n",
    "if possible for forecasting return the forecasted data\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"```python\\nimport pandas as pd\\nimport numpy as np\\nfrom statsmodels.tsa.statespace. sarimax import SARIMAX\\nfrom pmdarima import auto_arima\\n\\n# Read the data\\ndf = pd.read_csv('Mall_Customers.csv')\\n\\n# Check if the data is suitable for time series analysis\\nif 'Date' not in df.columns:\\n    print('False')\\n    exit()\\n\\n# Set the index to the Date column\\ndf.set_index('Date', inplace=True)\\n\\n# Create a time series plot\\ndf['Total Amount'].plot()\\nplt.xlabel('Date')\\nplt.ylabel('Total Amount')\\nplt.title('Time Series Plot of Total Amount')\\nplt.savefig('time_series.png')\\n\\n# Forecast next month sales using pmd autoarima\\nmodel = auto_arima(df['Total Amount'], seasonal=True)\\nforecast = model.predict(n_periods=30)\\n\\n# Save the forecast data\\nforecast.to_csv('forecast.csv')\\n```\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-b9c8806d-6b03-46c4-9df4-18955bb17aee-0' usage_metadata={'input_tokens': 399, 'output_tokens': 229, 'total_tokens': 628}\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from statsmodels.tsa.statespace. sarimax import SARIMAX\n",
      "from pmdarima import auto_arima\n",
      "\n",
      "# Read the data\n",
      "df = pd.read_csv('Mall_Customers.csv')\n",
      "\n",
      "# Check if the data is suitable for time series analysis\n",
      "if 'Date' not in df.columns:\n",
      "    print('False')\n",
      "    exit()\n",
      "\n",
      "# Set the index to the Date column\n",
      "df.set_index('Date', inplace=True)\n",
      "\n",
      "# Create a time series plot\n",
      "df['Total Amount'].plot()\n",
      "plt.xlabel('Date')\n",
      "plt.ylabel('Total Amount')\n",
      "plt.title('Time Series Plot of Total Amount')\n",
      "plt.savefig('time_series.png')\n",
      "\n",
      "# Forecast next month sales using pmd autoarima\n",
      "model = auto_arima(df['Total Amount'], seasonal=True)\n",
      "forecast = model.predict(n_periods=30)\n",
      "\n",
      "# Save the forecast data\n",
      "forecast.to_csv('forecast.csv')\n",
      "\n",
      "NameError(\"name 'exit' is not defined\")\n",
      "content=\"```python\\nimport pandas as pd\\nimport numpy as np\\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\\nfrom pmdarima import auto_arima\\nimport matplotlib.pyplot as plt\\n\\n# Read the data\\ndf = pd.read_csv('Mall_Customers.csv')\\n\\n# Check if the data is suitable for time series analysis\\nif 'Date' not in df.columns:\\n    print('False')\\nelse:\\n    # Set the index to the Date column\\n    df.set_index('Date', inplace=True)\\n\\n    # Create a time series plot\\n    df['Total Amount'].plot()\\n    plt.xlabel('Date')\\n    plt.ylabel('Total Amount')\\n    plt.title('Time Series Plot of Total Amount')\\n    plt.savefig('time_series.png')\\n\\n    # Forecast next month sales using pmd autoarima\\n    model = auto_arima(df['Total Amount'], seasonal=True)\\n    forecast = model.predict(n_periods=30)\\n\\n    # Save the forecast data\\n    forecast.to_csv('forecast.csv')\\n```\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-f34061e2-7faf-42c4-978c-bcc65e754176-0' usage_metadata={'input_tokens': 654, 'output_tokens': 249, 'total_tokens': 903}\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
      "from pmdarima import auto_arima\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Read the data\n",
      "df = pd.read_csv('Mall_Customers.csv')\n",
      "\n",
      "# Check if the data is suitable for time series analysis\n",
      "if 'Date' not in df.columns:\n",
      "    print('False')\n",
      "else:\n",
      "    # Set the index to the Date column\n",
      "    df.set_index('Date', inplace=True)\n",
      "\n",
      "    # Create a time series plot\n",
      "    df['Total Amount'].plot()\n",
      "    plt.xlabel('Date')\n",
      "    plt.ylabel('Total Amount')\n",
      "    plt.title('Time Series Plot of Total Amount')\n",
      "    plt.savefig('time_series.png')\n",
      "\n",
      "    # Forecast next month sales using pmd autoarima\n",
      "    model = auto_arima(df['Total Amount'], seasonal=True)\n",
      "    forecast = model.predict(n_periods=30)\n",
      "\n",
      "    # Save the forecast data\n",
      "    forecast.to_csv('forecast.csv')\n",
      "\n",
      "False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = code_executor(\"file location: data.csv\"+prompt)\n",
    "except:\n",
    "    result = 'hi\\n'\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'False'"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "if result.strip()==False:\n",
    "    abstract_data['forecasting'] = 'not possible'\n",
    "else:\n",
    "    abstract_data['forecasting'] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aprori():\n",
    "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    df_encoded = pd.get_dummies(df[categorical_columns], drop_first=True)\n",
    "\n",
    "    df_encoded.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    frequent_itemsets = apriori(df_encoded, min_support=0.05, use_colnames=True)\n",
    "\n",
    "    rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "\n",
    "    print(rules)\n",
    "    return rules\n",
    "\n",
    "    rules.to_csv('association_rules_output.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      antecedents                     consequents  \\\n",
      "0                   (Gender_Male)     (Product Category_Clothing)   \n",
      "1     (Product Category_Clothing)                   (Gender_Male)   \n",
      "2                   (Gender_Male)  (Product Category_Electronics)   \n",
      "3  (Product Category_Electronics)                   (Gender_Male)   \n",
      "\n",
      "   antecedent support  consequent support  support  confidence      lift  \\\n",
      "0               0.490               0.351    0.177    0.361224  1.029130   \n",
      "1               0.351               0.490    0.177    0.504274  1.029130   \n",
      "2               0.490               0.342    0.172    0.351020  1.026375   \n",
      "3               0.342               0.490    0.172    0.502924  1.026375   \n",
      "\n",
      "   leverage  conviction  zhangs_metric  \n",
      "0   0.00501    1.016006       0.055500  \n",
      "1   0.00501    1.028793       0.043613  \n",
      "2   0.00442    1.013899       0.050388  \n",
      "3   0.00442    1.026000       0.039054  \n"
     ]
    }
   ],
   "source": [
    "abstract_data['association analysis by apriori algorithm'] = aprori()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Top few samples of data':    Transaction ID  Gender   Age Product Category  Quantity  Price per Unit  \\\n",
       " 0             1.0    Male  34.0           Beauty       3.0            50.0   \n",
       " 1             2.0  Female  26.0         Clothing       2.0           500.0   \n",
       " 2             3.0    Male  50.0      Electronics       1.0            30.0   \n",
       " 3             4.0    Male  37.0         Clothing       1.0           500.0   \n",
       " 4             5.0    Male  30.0           Beauty       2.0            50.0   \n",
       " \n",
       "    Total Amount  \n",
       " 0         150.0  \n",
       " 1        1000.0  \n",
       " 2          30.0  \n",
       " 3         500.0  \n",
       " 4         100.0  ,\n",
       " 'Columns present in data': ['Transaction ID',\n",
       "  'Gender',\n",
       "  'Age',\n",
       "  'Product Category',\n",
       "  'Quantity',\n",
       "  'Price per Unit',\n",
       "  'Total Amount'],\n",
       " 'description about each and every columns': ['About Transaction ID \\ncount    1000.000000\\nmean      500.500000\\nstd       288.819436\\nmin         1.000000\\n25%       250.750000\\n50%       500.500000\\n75%       750.250000\\nmax      1000.000000\\nName: Transaction ID, dtype: float64\\nUnique values:1000',\n",
       "  'About Gender \\ncount       1000\\nunique         2\\ntop       Female\\nfreq         510\\nName: Gender, dtype: object\\nUnique values:2',\n",
       "  'About Age \\ncount    1000.00000\\nmean       41.39200\\nstd        13.68143\\nmin        18.00000\\n25%        29.00000\\n50%        42.00000\\n75%        53.00000\\nmax        64.00000\\nName: Age, dtype: float64\\nUnique values:47',\n",
       "  'About Product Category \\ncount         1000\\nunique           3\\ntop       Clothing\\nfreq           351\\nName: Product Category, dtype: object\\nUnique values:3',\n",
       "  'About Quantity \\ncount    1000.000000\\nmean        2.514000\\nstd         1.132734\\nmin         1.000000\\n25%         1.000000\\n50%         3.000000\\n75%         4.000000\\nmax         4.000000\\nName: Quantity, dtype: float64\\nUnique values:4',\n",
       "  'About Price per Unit \\ncount    1000.000000\\nmean      179.890000\\nstd       189.681356\\nmin        25.000000\\n25%        30.000000\\n50%        50.000000\\n75%       300.000000\\nmax       500.000000\\nName: Price per Unit, dtype: float64\\nUnique values:5',\n",
       "  'About Total Amount \\ncount    1000.000000\\nmean      456.000000\\nstd       559.997632\\nmin        25.000000\\n25%        60.000000\\n50%       135.000000\\n75%       900.000000\\nmax      2000.000000\\nName: Total Amount, dtype: float64\\nUnique values:18'],\n",
       " 'coefficient of variations': ['Coefficient of Variation of Transaction ID is : 0.5771',\n",
       "  'Coefficient of Variation of Age is : 0.3305',\n",
       "  'Coefficient of Variation of Quantity is : 0.4506',\n",
       "  'Coefficient of Variation of Price per Unit is : 1.0544',\n",
       "  'Coefficient of Variation of Total Amount is : 1.2281'],\n",
       " 'Summary of the data':    Unnamed: 0  No                                       Variable  \\\n",
       " 0           0   1            <strong>Gender</strong><br>[object]   \n",
       " 1           1   2  <strong>Product Category</strong><br>[object]   \n",
       " 2           2   3           <strong>Quantity</strong><br>[int64]   \n",
       " 3           3   4     <strong>Price per Unit</strong><br>[int64]   \n",
       " \n",
       "                                       Stats / Values  \\\n",
       " 0                               1. Female<br>2. Male   \n",
       " 1         1. Clothing<br>2. Electronics<br>3. Beauty   \n",
       " 2  Mean (sd) : 2.5 (1.1)<br>min < med < max:<br>1...   \n",
       " 3  Mean (sd) : 179.9 (189.7)<br>min < med < max:<...   \n",
       " \n",
       "                         Freqs / (% of Valid)  \\\n",
       " 0                 510 (51.0%)<br>490 (49.0%)   \n",
       " 1  351 (35.1%)<br>342 (34.2%)<br>307 (30.7%)   \n",
       " 2                          4 distinct values   \n",
       " 3                          5 distinct values   \n",
       " \n",
       "                                                Graph      Missing  \n",
       " 0  <img src = \"data:image/png;base64, iVBORw0KGgo...  0<br>(0.0%)  \n",
       " 1  <img src = \"data:image/png;base64, iVBORw0KGgo...  0<br>(0.0%)  \n",
       " 2  <img src = \"data:image/png;base64, iVBORw0KGgo...  0<br>(0.0%)  \n",
       " 3  <img src = \"data:image/png;base64, iVBORw0KGgo...  0<br>(0.0%)  ,\n",
       " 'chi-square contigency test for features': [' feature 1:Gender and feature 2: Product Category\\n        Chi-Square Statistic: 1.673837085800602\\n        p-value: 0.43304287262068974\\n        Degrees of freedom:2\\n        Expected frequencies:[[156.57 179.01 174.42]\\n [150.43 171.99 167.58]]\\nFail to reject H0 due to lack of evidence under significance level 0.05 Gender & Product Category are independent.'],\n",
       " 'Shapiro wilkinson normality test': ['Shapiro wilkinson normality test',\n",
       "  \"\\nRejected H0 under significance level 0.05\\nTransaction ID doesn't seems to be normally distributed\",\n",
       "  \"\\nRejected H0 under significance level 0.05\\nAge doesn't seems to be normally distributed\",\n",
       "  \"\\nRejected H0 under significance level 0.05\\nQuantity doesn't seems to be normally distributed\",\n",
       "  \"\\nRejected H0 under significance level 0.05\\nPrice per Unit doesn't seems to be normally distributed\",\n",
       "  \"\\nRejected H0 under significance level 0.05\\nTotal Amount doesn't seems to be normally distributed\"],\n",
       " 'Correaltions information about features': 0.857438500286872,\n",
       " 'Grouping among features': {'aggregations  & grouping by Gender':         Gender                 Product Category                  \n",
       "             max    mode     min              max      mode     min\n",
       "  Gender                                                           \n",
       "  Female  Female  Female  Female      Electronics  Clothing  Beauty\n",
       "  Male      Male    Male    Male      Electronics  Clothing  Beauty,\n",
       "  'aggregations & grouping by  Gender':        Transaction ID                      Age              Quantity       \\\n",
       "                   mean     max count       mean   max count      mean  max   \n",
       "  Gender                                                                      \n",
       "  Female     497.560784   999.0   510  41.356863  64.0   510  2.545098  4.0   \n",
       "  Male       503.559184  1000.0   490  41.428571  64.0   490  2.481633  4.0   \n",
       "  \n",
       "               Price per Unit              Total Amount                \n",
       "         count           mean    max count         mean     max count  \n",
       "  Gender                                                               \n",
       "  Female   510     180.068627  500.0   510   456.549020  2000.0   510  \n",
       "  Male     490     179.704082  500.0   490   455.428571  2000.0   490  ,\n",
       "  'aggregations  & grouping by Product Category':                  Gender                 Product Category               \\\n",
       "                      max    mode     min              max         mode   \n",
       "  Product Category                                                        \n",
       "  Beauty             Male  Female  Female           Beauty       Beauty   \n",
       "  Clothing           Male    Male  Female         Clothing     Clothing   \n",
       "  Electronics        Male    Male  Female      Electronics  Electronics   \n",
       "  \n",
       "                                 \n",
       "                            min  \n",
       "  Product Category               \n",
       "  Beauty                 Beauty  \n",
       "  Clothing             Clothing  \n",
       "  Electronics       Electronics  ,\n",
       "  'aggregations & grouping by  Product Category':                  Transaction ID                      Age              \\\n",
       "                             mean     max count       mean   max count   \n",
       "  Product Category                                                       \n",
       "  Beauty               491.413681   998.0   307  40.371336  64.0   307   \n",
       "  Clothing             494.943020   996.0   351  41.948718  64.0   351   \n",
       "  Electronics          514.359649  1000.0   342  41.736842  64.0   342   \n",
       "  \n",
       "                    Quantity            Price per Unit               \\\n",
       "                        mean  max count           mean    max count   \n",
       "  Product Category                                                    \n",
       "  Beauty            2.511401  4.0   307     184.055375  500.0   307   \n",
       "  Clothing          2.547009  4.0   351     174.287749  500.0   351   \n",
       "  Electronics       2.482456  4.0   342     181.900585  500.0   342   \n",
       "  \n",
       "                   Total Amount                \n",
       "                           mean     max count  \n",
       "  Product Category                             \n",
       "  Beauty             467.475570  2000.0   307  \n",
       "  Clothing           443.247863  2000.0   351  \n",
       "  Electronics        458.786550  2000.0   342  },\n",
       " 'Outliers points present in data': {'Transaction ID': {},\n",
       "  'Gender': {},\n",
       "  'Age': {},\n",
       "  'Product Category': {},\n",
       "  'Quantity': {},\n",
       "  'Price per Unit': {},\n",
       "  'Total Amount': {},\n",
       "  'outlier': {}},\n",
       " 'clusters': {'Gender': {0: 1, 1: 0, 2: 0, 3: 1, 4: 1},\n",
       "  'Product Category': {0: 1, 1: 1, 2: 1, 3: 2, 4: 1}},\n",
       " 'interpretaions among cluster segments':        Transaction ID        Age  Quantity  Price per Unit  Total Amount\n",
       " label                                                                   \n",
       " 0          514.501672  41.923077  2.428094       35.969900     87.491639\n",
       " 1          464.932961  40.296089  2.776536      410.614525   1106.703911\n",
       " 2          515.205438  41.930514  2.419940       55.392749    104.954683\n",
       " 3          461.767123  41.260274  3.054795      500.000000   1527.397260\n",
       " 4          501.686441  40.279661  2.262712      345.762712    724.576271,\n",
       " 'forecasting': 'False\\n',\n",
       " 'association analysis by apriori algorithm':                       antecedents                     consequents  \\\n",
       " 0                   (Gender_Male)     (Product Category_Clothing)   \n",
       " 1     (Product Category_Clothing)                   (Gender_Male)   \n",
       " 2                   (Gender_Male)  (Product Category_Electronics)   \n",
       " 3  (Product Category_Electronics)                   (Gender_Male)   \n",
       " \n",
       "    antecedent support  consequent support  support  confidence      lift  \\\n",
       " 0               0.490               0.351    0.177    0.361224  1.029130   \n",
       " 1               0.351               0.490    0.177    0.504274  1.029130   \n",
       " 2               0.490               0.342    0.172    0.351020  1.026375   \n",
       " 3               0.342               0.490    0.172    0.502924  1.026375   \n",
       " \n",
       "    leverage  conviction  zhangs_metric  \n",
       " 0   0.00501    1.016006       0.055500  \n",
       " 1   0.00501    1.028793       0.043613  \n",
       " 2   0.00442    1.013899       0.050388  \n",
       " 3   0.00442    1.026000       0.039054  }"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_text(data, filename='data_results.txt'):\n",
    "    with open(filename, 'w') as file:\n",
    "        for key, value in data.items():\n",
    "            file.write(f\"{key}:\\n\")\n",
    "            if isinstance(value, list):\n",
    "                for item in value:\n",
    "                    file.write(f\"  - {item}\\n\")\n",
    "            elif isinstance(value, pd.DataFrame):\n",
    "                file.write(value.to_string(index=False))\n",
    "            elif isinstance(value, str):\n",
    "                for line in value.split('\\n'):\n",
    "                    file.write(f\"  {line}\\n\")\n",
    "            else:\n",
    "                file.write(f\"  {value}\\n\")\n",
    "\n",
    "\n",
    "convert_dict_to_text(abstract_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pandasai import SmartDataframe\n",
    "from pandasai.llm import GoogleGemini\n",
    "import pandas as pd\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.agents import create_pandas_dataframe_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(model = 'gpt-4o-mini')\n",
    "# \n",
    "# llm = GoogleGemini(api_key='AIzaSyAhUeySoblg5TFG37IHXuz_6mruby5GwvQ',temperature=0.4)\n",
    "\n",
    "file_path = r\"data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "# sm = SmartDataframe(file_path,config={'llm':llm,\"custom_whitelisted_dependencies\": [\"scikit-learn\"]})\n",
    "\n",
    "# response = sm.chat('give me the product distribution among sales')\n",
    "agent_executor = create_pandas_dataframe_agent(\n",
    "    llm,\n",
    "    df,\n",
    "    agent_type='tool-calling',\n",
    "    verbose=True,\n",
    "    allow_dangerous_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': \"df['Previous Purchases']\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m0       14\n",
      "1        2\n",
      "2       23\n",
      "3       49\n",
      "4       31\n",
      "        ..\n",
      "3895    32\n",
      "3896    41\n",
      "3897    24\n",
      "3898    24\n",
      "3899    33\n",
      "Name: Previous Purchases, Length: 3900, dtype: int64\u001b[0m\u001b[32;1m\u001b[1;3mThe data for \"Previous Purchases\" in the dataframe is as follows:\n",
      "\n",
      "```\n",
      "0       14\n",
      "1        2\n",
      "2       23\n",
      "3       49\n",
      "4       31\n",
      "...\n",
      "3895    32\n",
      "3896    41\n",
      "3897    24\n",
      "3898    24\n",
      "3899    33\n",
      "```\n",
      "\n",
      "This represents the number of previous purchases for each customer in the dataframe. The length of the data series is 3900 rows.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "('The data for \"Previous Purchases\" in the dataframe is as follows:\\n'\n",
      " '\\n'\n",
      " '```\\n'\n",
      " '0       14\\n'\n",
      " '1        2\\n'\n",
      " '2       23\\n'\n",
      " '3       49\\n'\n",
      " '4       31\\n'\n",
      " '...\\n'\n",
      " '3895    32\\n'\n",
      " '3896    41\\n'\n",
      " '3897    24\\n'\n",
      " '3898    24\\n'\n",
      " '3899    33\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'This represents the number of previous purchases for each customer in the '\n",
      " 'dataframe. The length of the data series is 3900 rows.')\n"
     ]
    }
   ],
   "source": [
    "pprint(agent_executor.invoke('display previous purchases data')['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': 'df.columns.tolist()'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['Unnamed: 0', 'Customer ID', 'Age', 'Gender', 'Item Purchased', 'Category', 'Purchase Amount (USD)', 'Location', 'Size', 'Color', 'Season', 'Review Rating', 'Subscription Status', 'Payment Method', 'Shipping Type', 'Discount Applied', 'Promo Code Used', 'Previous Purchases', 'Preferred Payment Method', 'Frequency of Purchases']\u001b[0m\u001b[32;1m\u001b[1;3mThe feature names in the dataframe are:\n",
      "\n",
      "1. Unnamed: 0\n",
      "2. Customer ID\n",
      "3. Age\n",
      "4. Gender\n",
      "5. Item Purchased\n",
      "6. Category\n",
      "7. Purchase Amount (USD)\n",
      "8. Location\n",
      "9. Size\n",
      "10. Color\n",
      "11. Season\n",
      "12. Review Rating\n",
      "13. Subscription Status\n",
      "14. Payment Method\n",
      "15. Shipping Type\n",
      "16. Discount Applied\n",
      "17. Promo Code Used\n",
      "18. Previous Purchases\n",
      "19. Preferred Payment Method\n",
      "20. Frequency of Purchases\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'dispaly the feature names',\n",
       " 'output': 'The feature names in the dataframe are:\\n\\n1. Unnamed: 0\\n2. Customer ID\\n3. Age\\n4. Gender\\n5. Item Purchased\\n6. Category\\n7. Purchase Amount (USD)\\n8. Location\\n9. Size\\n10. Color\\n11. Season\\n12. Review Rating\\n13. Subscription Status\\n14. Payment Method\\n15. Shipping Type\\n16. Discount Applied\\n17. Promo Code Used\\n18. Previous Purchases\\n19. Preferred Payment Method\\n20. Frequency of Purchases'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke('dispaly the feature names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': \"df['Category'].value_counts()\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mCategory\n",
      "Clothing       1737\n",
      "Accessories    1240\n",
      "Footwear        599\n",
      "Outerwear       324\n",
      "Name: count, dtype: int64\u001b[0m\u001b[32;1m\u001b[1;3mThe distribution of products in the dataframe is as follows:\n",
      "\n",
      "- **Clothing**: 1737 products\n",
      "- **Accessories**: 1240 products\n",
      "- **Footwear**: 599 products\n",
      "- **Outerwear**: 324 products\n",
      "\n",
      "Clothing is the most common product category, followed by Accessories, Footwear, and Outerwear.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "('The distribution of products in the dataframe is as follows:\\n'\n",
      " '\\n'\n",
      " '- **Clothing**: 1737 products\\n'\n",
      " '- **Accessories**: 1240 products\\n'\n",
      " '- **Footwear**: 599 products\\n'\n",
      " '- **Outerwear**: 324 products\\n'\n",
      " '\\n'\n",
      " 'Clothing is the most common product category, followed by Accessories, '\n",
      " 'Footwear, and Outerwear.')\n"
     ]
    }
   ],
   "source": [
    "pprint(agent_executor.invoke('tells about the distribution of products')['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': \"male_product_counts = df[df['Gender'] == 'Male']['Item Purchased'].value_counts()\\nmale_most_purchased_product = male_product_counts.idxmax()\\nmale_most_purchased_product\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPants\u001b[0m\u001b[32;1m\u001b[1;3mThe product that male users bought the most is \"Pants.\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'which product did male user bought mostly',\n",
       " 'output': 'The product that male users bought the most is \"Pants.\"'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke('which product did male user bought mostly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': \"df[['Customer ID', 'Purchase Amount (USD)']].groupby('Customer ID').sum().sort_values('Purchase Amount (USD)', ascending=False).head(1)\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m             Purchase Amount (USD)\n",
      "Customer ID                       \n",
      "2843                           100\u001b[0m\u001b[32;1m\u001b[1;3mThe customer who contributes the most profit to the business is Customer ID 2843, with a total purchase amount of 100 USD.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'which customer contributes more profit to the business',\n",
       " 'output': 'The customer who contributes the most profit to the business is Customer ID 2843, with a total purchase amount of 100 USD.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke('which customer contributes more profit to the business')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_rag import rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it appears that the data includes a summary of the 'gender' category. \n",
      "\n",
      "End to end analysis: Unfortunately, the data provided is incomplete, so I'll have to perform an end-to-end analysis to provide more insights.\n",
      "\n",
      "To proceed, I'll make some assumptions and perform a hypothetical analysis. \n",
      "\n",
      "If we were to assume a typical dataset with gender information, we might expect to see a summary of the counts for each gender category. \n",
      "\n",
      "For example, if the dataset had around 1000 entries, we might see a summary like this:\n",
      "\n",
      "- Male: 550 (55%)\n",
      "- Female: 450 (45%)\n",
      "\n",
      "However, without more information, it's impossible to determine the actual distribution of the data. \n",
      "\n",
      "If you provide more data, I can perform a more accurate analysis.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "question = 'analyse the data'\n",
    "response = chain.invoke({'input':question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=response[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandasai import SmartDataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SmartDataframe(file_path,config={'llm':llm,\"custom_whitelisted_dependencies\": [\"scikit-learn\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:/Users/navab/Downloads/sem_V_test/exports/charts/temp_chart.png'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.chat(\"compare and contrast the sales of pant between women and men\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': \"pant_sales = df[df['Item Purchased'] == 'Pants']\\n\\n# Group by Gender and calculate total sales and average purchase amount\\ncomparison = pant_sales.groupby('Gender')['Purchase Amount (USD)'].agg(['sum', 'mean'])\\ncomparison\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m         sum       mean\n",
      "Gender                 \n",
      "Female  2936  61.166667\n",
      "Male    7154  58.162602\u001b[0m\u001b[32;1m\u001b[1;3mHere's the comparison of pant sales between men and women based on the provided data:\n",
      "\n",
      "- **Total Sales**:\n",
      "  - **Female**: $2,936\n",
      "  - **Male**: $7,154\n",
      "\n",
      "- **Average Purchase Amount**:\n",
      "  - **Female**: $61.17\n",
      "  - **Male**: $58.16\n",
      "\n",
      "### Summary:\n",
      "- Total sales of pants are significantly higher for men compared to women.\n",
      "- However, the average purchase amount for females is slightly higher than that for males. \n",
      "\n",
      "This indicates that while men purchase more pants overall, women tend to spend a bit more per purchase on average.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'compare and contrast the sales of pant between women and men',\n",
       " 'output': \"Here's the comparison of pant sales between men and women based on the provided data:\\n\\n- **Total Sales**:\\n  - **Female**: $2,936\\n  - **Male**: $7,154\\n\\n- **Average Purchase Amount**:\\n  - **Female**: $61.17\\n  - **Male**: $58.16\\n\\n### Summary:\\n- Total sales of pants are significantly higher for men compared to women.\\n- However, the average purchase amount for females is slightly higher than that for males. \\n\\nThis indicates that while men purchase more pants overall, women tend to spend a bit more per purchase on average.\"}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\"compare and contrast the sales of pant between women and men\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': \"import pandas as pd\\n\\n# Assuming df is already defined and is our dataframe\\n\\n# Group by 'Item Purchased' and calculate average review rating and total purchase amounts\\nproduct_analysis = df.groupby('Item Purchased').agg({\\n    'Review Rating': 'mean',\\n    'Purchase Amount (USD)': 'sum',\\n    'Frequency of Purchases': 'count'\\n}).reset_index()\\n\\n# Rename columns for clarity\\nproduct_analysis.columns = ['Item Purchased', 'Average Review Rating', 'Total Purchase Amount (USD)', 'Total Purchases']\\n\\n# Filter products with low average ratings and low total purchase amounts\\n# Setting thresholds: Average rating below 3.0 and total purchases below a certain amount\\nthreshold_rating = 3.0\\nthreshold_amount = 100  # You can adjust this threshold as needed\\n\\nlow_performing_products = product_analysis[(product_analysis['Average Review Rating'] < threshold_rating) | \\n                                          (product_analysis['Total Purchase Amount (USD)'] < threshold_amount)]\\n\\nlow_performing_products\"}`\n",
      "responded: To determine which products should potentially be removed from sales, we can analyze the dataframe for factors such as low review ratings and low purchase amounts. Generally, products with low ratings (e.g., below 3.0) or low sales figures might not be performing well and could be considered for removal.\n",
      "\n",
      "Let's compute the average review rating and total purchase amount for each item. Products with a consistently low review rating or low total purchase amounts can be flagged for review or potential removal.\n",
      "\n",
      "I'll start by aggregating the data.\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mEmpty DataFrame\n",
      "Columns: [Item Purchased, Average Review Rating, Total Purchase Amount (USD), Total Purchases]\n",
      "Index: []\u001b[0m\u001b[32;1m\u001b[1;3mIt appears that there are no products with an average review rating below 3.0 or a total purchase amount below the specified threshold of 100 USD, based on the current data in the dataframe.\n",
      "\n",
      "Since we found no low-performing products, it may indicate that all items are performing reasonably well. However, if you'd like to explore other criteria for product removal, such as specific items with low ratings or infrequent purchases, please let me know!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'which product have to be removed from sales',\n",
       " 'output': \"It appears that there are no products with an average review rating below 3.0 or a total purchase amount below the specified threshold of 100 USD, based on the current data in the dataframe.\\n\\nSince we found no low-performing products, it may indicate that all items are performing reasonably well. However, if you'd like to explore other criteria for product removal, such as specific items with low ratings or infrequent purchases, please let me know!\"}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke('which product have to be removed from sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo provide inventory suggestions based on the data in the dataframe, we can analyze the following factors:\n",
      "\n",
      "1. **Sales Performance**: Identify best-selling products by analyzing the `Purchase Amount (USD)` and the frequency of purchases.\n",
      "\n",
      "2. **Customer Preferences**: Look at the `Size`, `Color`, and `Category` to understand what type of products are most preferred by customers.\n",
      "\n",
      "3. **Seasonality**: Analyze the `Season` to determine which items should be stocked in anticipation of seasonal demand.\n",
      "\n",
      "4. **Discount Potential**: Explore the `Discount Applied` and `Promo Code Used` columns to understand which products may benefit from targeted promotions to boost sales.\n",
      "\n",
      "Here are some specific inventory suggestions:\n",
      "\n",
      "### Inventory Suggestions\n",
      "\n",
      "1. **Increase Stock of Best-Sellers**: \n",
      "   - Identify the items with the highest `Purchase Amount (USD)` and ensure stock levels are sufficient to meet demand.\n",
      "\n",
      "2. **Optimize Size and Color Offerings**:\n",
      "   - Based on the distribution of `Size` and `Color`, stock more of the most popular sizes and colors to cater to customer preferences.\n",
      "\n",
      "3. **Seasonal Inventory Planning**:\n",
      "   - Prepare inventory for seasonal items (e.g., more clothing in Spring/Summer).\n",
      "   - Ensure winter items like `Sweaters` and `Blouses` are stocked well ahead of the colder months.\n",
      "\n",
      "4. **Bundle Offers**: \n",
      "   - Create bundle offers for items frequently purchased together. This can leverage `Previous Purchases` data to identify patterns.\n",
      "\n",
      "5. **Promotions and Discounts**: \n",
      "   - For items with high `Review Rating` that are not selling as expected, consider targeted promotions or discounts to increase their visibility and sales.\n",
      "\n",
      "6. **Monitor Unsold Inventory**: \n",
      "   - Regularly review items with low sales and consider reducing prices or changing their categorizations based on customer feedback.\n",
      "\n",
      "7. **Track Subscription Trends**:\n",
      "   - Items popular among customers with `Subscription Status` should be prioritized for consistent stocking.\n",
      "\n",
      "By analyzing the data further, we can make more tailored recommendations. If you'd like, I can perform some analysis on the dataframe to provide more detailed insights.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'give me some inventry suggestions about the products in data',\n",
       " 'output': \"To provide inventory suggestions based on the data in the dataframe, we can analyze the following factors:\\n\\n1. **Sales Performance**: Identify best-selling products by analyzing the `Purchase Amount (USD)` and the frequency of purchases.\\n\\n2. **Customer Preferences**: Look at the `Size`, `Color`, and `Category` to understand what type of products are most preferred by customers.\\n\\n3. **Seasonality**: Analyze the `Season` to determine which items should be stocked in anticipation of seasonal demand.\\n\\n4. **Discount Potential**: Explore the `Discount Applied` and `Promo Code Used` columns to understand which products may benefit from targeted promotions to boost sales.\\n\\nHere are some specific inventory suggestions:\\n\\n### Inventory Suggestions\\n\\n1. **Increase Stock of Best-Sellers**: \\n   - Identify the items with the highest `Purchase Amount (USD)` and ensure stock levels are sufficient to meet demand.\\n\\n2. **Optimize Size and Color Offerings**:\\n   - Based on the distribution of `Size` and `Color`, stock more of the most popular sizes and colors to cater to customer preferences.\\n\\n3. **Seasonal Inventory Planning**:\\n   - Prepare inventory for seasonal items (e.g., more clothing in Spring/Summer).\\n   - Ensure winter items like `Sweaters` and `Blouses` are stocked well ahead of the colder months.\\n\\n4. **Bundle Offers**: \\n   - Create bundle offers for items frequently purchased together. This can leverage `Previous Purchases` data to identify patterns.\\n\\n5. **Promotions and Discounts**: \\n   - For items with high `Review Rating` that are not selling as expected, consider targeted promotions or discounts to increase their visibility and sales.\\n\\n6. **Monitor Unsold Inventory**: \\n   - Regularly review items with low sales and consider reducing prices or changing their categorizations based on customer feedback.\\n\\n7. **Track Subscription Trends**:\\n   - Items popular among customers with `Subscription Status` should be prioritized for consistent stocking.\\n\\nBy analyzing the data further, we can make more tailored recommendations. If you'd like, I can perform some analysis on the dataframe to provide more detailed insights.\"}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke('give me some inventry suggestions about the products in data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHere are five exploratory data analysis (EDA) questions you can consider analyzing from the given dataframe:\n",
      "\n",
      "1. **Customer Demographics Analysis:**\n",
      "   - What is the distribution of customers by age and gender? Are there significant differences in purchase behavior between different age groups and genders?\n",
      "\n",
      "2. **Purchase Behavior Analysis:**\n",
      "   - What are the most commonly purchased items and their corresponding categories? How does the purchase amount vary across different categories (Clothing vs. Footwear)?\n",
      "\n",
      "3. **Location-Based Insights:**\n",
      "   - Which locations have the highest average purchase amounts? Are there any trends or patterns in spending that can be identified based on geographic location?\n",
      "\n",
      "4. **Seasonal Trends:**\n",
      "   - How does the purchase amount vary across different seasons? Is there a certain season where purchases tend to be higher, and does this differ for different product categories?\n",
      "\n",
      "5. **Payment and Shipping Preferences:**\n",
      "   - What are the preferred payment methods and shipping types among customers? Are there any correlations between payment methods, shipping types, and customer satisfaction as indicated by review ratings?\n",
      "\n",
      "These questions can provide insightful information about customer behavior, preferences, and trends within the dataset.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'give me top 5 eda questions to be analysed in data',\n",
       " 'output': 'Here are five exploratory data analysis (EDA) questions you can consider analyzing from the given dataframe:\\n\\n1. **Customer Demographics Analysis:**\\n   - What is the distribution of customers by age and gender? Are there significant differences in purchase behavior between different age groups and genders?\\n\\n2. **Purchase Behavior Analysis:**\\n   - What are the most commonly purchased items and their corresponding categories? How does the purchase amount vary across different categories (Clothing vs. Footwear)?\\n\\n3. **Location-Based Insights:**\\n   - Which locations have the highest average purchase amounts? Are there any trends or patterns in spending that can be identified based on geographic location?\\n\\n4. **Seasonal Trends:**\\n   - How does the purchase amount vary across different seasons? Is there a certain season where purchases tend to be higher, and does this differ for different product categories?\\n\\n5. **Payment and Shipping Preferences:**\\n   - What are the preferred payment methods and shipping types among customers? Are there any correlations between payment methods, shipping types, and customer satisfaction as indicated by review ratings?\\n\\nThese questions can provide insightful information about customer behavior, preferences, and trends within the dataset.'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke('give me top 5 eda questions to be analysed in data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Most Common Items</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Blouse</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jewelry</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pants</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shirt</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dress</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sweater</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jacket</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Belt</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sunglasses</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Coat</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Most Common Items  Frequency\n",
       "0            Blouse        171\n",
       "1           Jewelry        171\n",
       "2             Pants        171\n",
       "3             Shirt        169\n",
       "4             Dress        166\n",
       "5           Sweater        164\n",
       "6            Jacket        163\n",
       "7              Belt        161\n",
       "8        Sunglasses        161\n",
       "9              Coat        161"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.chat('What are the most commonly purchased items and their corresponding categories? How does the purchase amount vary across different categories (Clothing vs. Footwear)?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_rag import rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_community.document_loaders import JSONLoader,CSVLoader\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# def rag():\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\",google_api_key='AIzaSyAhUeySoblg5TFG37IHXuz_6mruby5GwvQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=\"mixedbread-ai/mxbai-embed-large-v1\",\n",
    "#     )   \n",
    "\n",
    "loader = CSVLoader(r\"data_results.txt\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# vector_store = InMemoryVectorStore(embeddings)\n",
    "# _ = vector_store.add_documents(documents)\n",
    "vector_store = FAISS.from_documents(documents,embeddings)\n",
    "retreiver = vector_store.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system = (\"\"\" \n",
    "You are a data scientist tasked with analyzing the provided dataset. Your goal is to extract meaningful insights, identify patterns, and generate a comprehensive analytical report. Follow the structure below to ensure the analysis is thorough and actionable:\n",
    "\n",
    "1. **Data Understanding**  \n",
    "   - Describe the dataset (e.g., size, features, data types, missing values, etc.).  \n",
    "   - Highlight any initial observations or challenges.  \n",
    "\n",
    "2. **What Was in the Data?**  \n",
    "   - Summarize the key attributes, trends, and distributions in the dataset.  \n",
    "   - Identify any notable patterns, outliers, or anomalies.  \n",
    "\n",
    "3. **How and Why It Happened**  \n",
    "   - Explain the underlying reasons for the observed trends or patterns.  \n",
    "   - Provide context or hypotheses for why certain phenomena occurred.  \n",
    "\n",
    "4. **Beautiful Insights**  \n",
    "   - Share unique, non-obvious, or visually appealing insights derived from the data.  \n",
    "   - Use visualizations or examples to support your findings.  \n",
    "\n",
    "5. **Takeaways**  \n",
    "   - Summarize the most important findings from the analysis.  \n",
    "   - Highlight actionable insights or conclusions.  \n",
    "\n",
    "6. **Suggestions**  \n",
    "   - Provide recommendations based on the analysis (e.g., improvements, next steps, etc.).  \n",
    "\n",
    "7. **Future Perspectives**  \n",
    "   - Discuss potential future trends or areas for further investigation.  \n",
    "   - Suggest how the data can be leveraged for long-term value.  \n",
    "\n",
    "Ensure the report is concise, well-structured, and focused on delivering value. Use clear language and avoid unnecessary jargon. \"\"\")\n",
    "human = \"\"\"\n",
    "Analyze the provided dataset and generate a precise analytical report based on the following:  \n",
    "\n",
    "- **Question**: {question}  \n",
    "- **Context**: {context}  \n",
    "\n",
    "Provide a detailed response following the structure outlined in the system prompt. Focus on extracting actionable insights and making the data valuable.\"\"\"\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", system),\n",
    "    (\"human\", human),\n",
    "])\n",
    "\n",
    "chain = prompt |  llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chain.invoke({'question':'hi'}).content)\n",
    "def run(question):\n",
    "    contents = retreiver.invoke(question)\n",
    "    return chain.invoke({'question':question,'context':contents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('**Data Understanding**\\n'\n",
      " '\\n'\n",
      " 'The provided dataset, \"Customer Shopping Data,\" contains information on '\n",
      " 'customer shopping trends. The dataset is composed of numerical and '\n",
      " 'categorical features, with various data types, including integers, floats, '\n",
      " 'and strings. After reviewing the provided summary, I noticed that the '\n",
      " 'dataset has 410 rows, but the row number in the metadata hints at a larger '\n",
      " 'dataset. Additionally, there is no information on missing values, which '\n",
      " 'might be an important aspect to consider in the analysis.\\n'\n",
      " '\\n'\n",
      " '**What Was in the Data?**\\n'\n",
      " '\\n'\n",
      " 'The provided summary does not explicitly mention the features in the '\n",
      " 'dataset. However, based on the context, I assume that the dataset includes '\n",
      " 'variables such as customer demographics, shopping behavior, and transaction '\n",
      " 'history. Further analysis is required to understand the actual features and '\n",
      " 'their distributions.\\n'\n",
      " '\\n'\n",
      " 'The summary mentions that statistical tests have been applied to the '\n",
      " 'dataset, including Chi-square tests for categorical variables and '\n",
      " 'Shapiro-Wilk tests for normality of numerical features. This suggests that '\n",
      " 'the dataset contains a mix of categorical and numerical variables.\\n'\n",
      " '\\n'\n",
      " '**How and Why It Happened**\\n'\n",
      " '\\n'\n",
      " 'The underlying reasons for the observed trends or patterns in the dataset '\n",
      " 'are not explicitly stated. However, based on the context, it can be inferred '\n",
      " 'that the dataset is a result of customer shopping behavior over a specific '\n",
      " 'period. The main variables included in the dataset likely capture various '\n",
      " 'aspects of customer behavior, such as demographics, shopping frequency, and '\n",
      " 'transaction amounts.\\n'\n",
      " '\\n'\n",
      " '**Beautiful Insights**\\n'\n",
      " '\\n'\n",
      " 'Unfortunately, due to the lack of specific feature information, I cannot '\n",
      " 'provide unique, non-obvious, or visually appealing insights derived from the '\n",
      " 'data. However, I can suggest that visualizing the distribution of key '\n",
      " 'features, such as customer demographics, shopping frequency, and transaction '\n",
      " 'amounts, could provide valuable insights into customer behavior.\\n'\n",
      " '\\n'\n",
      " '**Takeaways**\\n'\n",
      " '\\n'\n",
      " 'The main takeaway from this analysis is that the dataset, \"Customer Shopping '\n",
      " 'Data,\" contains information on customer shopping trends, with a mix of '\n",
      " 'categorical and numerical variables. The dataset has undergone statistical '\n",
      " 'testing to understand the distribution of the data. Further analysis is '\n",
      " 'required to understand the actual features and their distributions, as well '\n",
      " 'as to derive actionable insights.\\n'\n",
      " '\\n'\n",
      " '**Suggestions**\\n'\n",
      " '\\n'\n",
      " 'Based on the analysis, I suggest the following:\\n'\n",
      " '\\n'\n",
      " '1. Provide a detailed description of the features in the dataset, including '\n",
      " 'their data types and distributions.\\n'\n",
      " '2. Visualize the distribution of key features to gain insights into customer '\n",
      " 'behavior.\\n'\n",
      " '3. Apply statistical tests to identify correlations between variables and '\n",
      " 'understand the relationships between different features.\\n'\n",
      " '\\n'\n",
      " '**Future Perspectives**\\n'\n",
      " '\\n'\n",
      " 'The dataset has the potential to be leveraged for long-term value by '\n",
      " 'providing insights into customer behavior and shopping trends. By analyzing '\n",
      " 'the dataset, businesses can identify opportunities to improve customer '\n",
      " 'satisfaction, increase sales, and optimize their marketing strategies.\\n'\n",
      " '\\n'\n",
      " 'Overall, while the dataset provides a good starting point, further analysis '\n",
      " 'is required to extract actionable insights and make the data valuable. By '\n",
      " 'following the suggestions provided, businesses can unlock the potential of '\n",
      " 'this dataset and gain a competitive edge in the market.')\n"
     ]
    }
   ],
   "source": [
    "pprint(run('analyse the data').content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_rag import rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Do extract and intrepret the abstracted data and provide a super analytical report of the data which can help the client/user to take some immediate decision from your report\"\"\"\n",
    "result = rag_chain.invoke({\"input\":prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Do extract and intrepret the abstracted data and provide a super analytical report of the data which can help the client/user to take some immediate decision from your report',\n",
       " 'context': [Document(metadata={'source': 'data_results.txt', 'row': 0}, page_content=\"meta:: The abstracted portion of the data analysis is included here to be further analyzed by LLMs to generate reports. Quick analysis of this data is crucial for faster decision-making on the client's side. This summary includes descriptions of data\\nNone: feature types,feature correlations,statistical tests on univariate features,facts and cubes,and visualizations.\"),\n",
       "  Document(metadata={'source': 'data_results.txt', 'row': 36}, page_content='meta:: Aggregated data summaries\\nNone: including key metrics and dimensions used for analyzing patterns and trends. Facts and cubes provide a multi-dimensional view of the data,facilitating in-depth analysis and insights.'),\n",
       "  Document(metadata={'source': 'data_results.txt', 'row': 80}, page_content='meta:: it contains the data pints which are outliers please intrepret the those points in useful analysis')],\n",
       " 'answer': \"**Abstracted Data Analysis and Interpretation Report**\\n\\n**Data Overview**\\n\\nWe have analyzed the provided abstracted data, which includes feature types, feature correlations, statistical tests on univariate features, facts and cubes, and visualizations. Upon reviewing the data, we have identified key trends, patterns, and insights that can inform decision-making for the client.\\n\\n**Key Findings and Insights**\\n\\n1. **Feature Correlations**:\\n - The strongest correlations are observed between:\\n   - Feature A and Feature B (r = 0.85)\\n   - Feature C and Feature D (r = 0.78)\\n   - Feature E and Feature F (r = 0.69)\\n2. **Statistical Tests on Univariate Features**:\\n   - Feature G shows a significant positive skewness (K-S statistic: 0.05)\\n   - Feature H exhibits a moderate negative correlation with Feature I (r = -0.43)\\n3. **Facts and Cubes Analysis**:\\n   - The average value of Feature J across all dimensions is 42.17\\n   - The max value of Feature K across all dimensions is 95.23\\n   - The min value of Feature L across all dimensions is 10.15\\n4. **Visualizations**:\\n   - Feature M shows a clear bimodal distribution with peaks at 0.23 and 0.45\\n   - Feature N exhibits a strong linear relationship with Feature O\\n\\n**Outlier Analysis**\\n\\nUpon identifying outliers in the data points, we have analyzed their characteristics:\\n\\n1. **Outlier 1 (Feature P)**:\\n   - Value: 12.56 (3 standard deviations away from the mean)\\n   - Dimension: Feature Q (category: 'Category 3')\\n   - Observation: This outlier is likely an error or an anomaly in the data collection process.\\n2. **Outlier 2 (Feature R)**:\\n   - Value: 89.23 (4 standard deviations away from the mean)\\n   - Dimension: Feature S (category: 'Category 2')\\n   - Observation: This outlier may indicate a data point that does not conform to the expected behavior or pattern.\\n\\n**Recommendations for Immediate Decision-Making**\\n\\nBased on the analysis, we recommend the following actions:\\n\\n1. **Data Quality Check**: Investigate the cause of Outlier 1 and correct the data point to ensure accuracy and reliability.\\n2. **Further Exploration**: Analyze the relationship between Feature A and Feature B, as well as Feature C and Feature D, to understand the underlying mechanisms driving these correlations.\\n3. **Feature Engineering**: Consider transforming Feature G to reduce its skewness and improve data normality.\\n4. **Dimensionality Reduction**: Apply techniques like PCA or t-SNE to reduce the dimensionality of the data and facilitate easier analysis.\\n\\nBy implementing these recommendations, the client can improve data quality, gain deeper insights, and make more informed decisions.\"}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_rag import rag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Do extract and intrepret the abstracted data and provide a super analytical report of the data which can help the client/user to take some immediate decision from your report\"\"\"\n",
    "result = chain.invoke({\"input\":prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'which team did score most',\n",
       " 'context': [Document(metadata={'source': 'data_results.txt', 'row': 61}, page_content='summary:: Pakistan    Netherlands   Hyderabad    Netherlands   Ball             294                10                10       Pakistan                    81R        1'),\n",
       "  Document(metadata={'source': 'data_results.txt', 'row': 66}, page_content='summary:: Netherlands   South Africa Dharamshala   South Africa   Ball             258                 8                10    Netherlands                    38R        1'),\n",
       "  Document(metadata={'source': 'data_results.txt', 'row': 67}, page_content='summary:: New Zealand    Afghanistan     Chennai    Afghanistan   Ball             300                 6                10    New Zealand                   149R        1')],\n",
       " 'answer': 'Based on the data, New Zealand scored the most with 300 runs.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\":\"which team did score most\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Do extract and intrepret the abstracted data and provide a super analytical report of the data which can help the client/user to take some immediate decision from your report',\n",
       " 'context': [Document(metadata={'source': 'data_results.txt', 'row': 34}, page_content='summary:: Aggregated data summaries\\nNone: including key metrics and dimensions used for analyzing patterns and trends. Facts and cubes provide a multi-dimensional view of the data,facilitating in-depth analysis and insights.'),\n",
       "  Document(metadata={'source': 'data_results.txt', 'row': 78}, page_content='summary:: it contains the data pints which are outliers please intrepret the those points in useful analysis'),\n",
       "  Document(metadata={'source': 'data_results.txt', 'row': 41}, page_content='summary:: Stacked bar charts visualizing the distribution of categorical data across different segments. These visualizations help in understanding the composition and relative proportions of different categories.')],\n",
       " 'answer': \"**Executive Summary:**\\n\\nBased on the aggregated data summaries, key metrics, and dimensions, I have conducted a comprehensive analysis to extract insights and provide actionable recommendations for the client/user. The report highlights key trends, patterns, and outliers, enabling informed decision-making.\\n\\n**Key Findings:**\\n\\n1. **Outliers Analysis**: The data points identified as outliers are:\\n\\t* Customer ID: 00123 with a purchase value of $1,500 (3.5 standard deviations above the mean)\\n\\t* Customer ID: 04567 with a purchase frequency of 10 (2.5 standard deviations above the mean)\\n\\t* Product ID: P001 with a sales value of $10,000 (2.2 standard deviations above the mean)\\n\\nThese outliers indicate exceptional customers and products that warrant further investigation. Customer 00123 and 04567 may be high-value targets for loyalty programs or special offers, while Product P001 may be a top-selling item that requires optimization.\\n\\n2. **Segmentation Analysis**: The stacked bar charts reveal that:\\n\\t* Category A accounts for 35% of total sales, with 60% of customers purchasing from this category.\\n\\t* Category B represents 25% of total sales, with 30% of customers purchasing from this category.\\n\\t* Category C accounts for 20% of total sales, with 10% of customers purchasing from this category.\\n\\nThis analysis suggests that Category A has a strong customer base and may benefit from targeted marketing initiatives. Category B requires a more balanced marketing strategy, while Category C may need additional promotion to increase customer engagement.\\n\\n3. **Customer Behavior Analysis**: The data reveals that:\\n\\t* 75% of customers make repeat purchases within 30 days of their initial purchase.\\n\\t* 40% of customers purchase from multiple categories.\\n\\t* 20% of customers have been inactive for over 6 months.\\n\\nThese findings indicate that customers value convenience and may benefit from loyalty programs or reminders to stimulate repeat business. The inactive customer segment requires re-engagement strategies to prevent churn.\\n\\n**Recommendations:**\\n\\n1. **Targeted Marketing**: Develop targeted marketing campaigns for Category A customers, focusing on loyalty programs and exclusive offers.\\n2. **Product Optimization**: Analyze Product P001's sales performance and consider optimizing pricing, inventory, or packaging to increase sales.\\n3. **Customer Retention**: Implement a loyalty program or email reminders to encourage repeat business from inactive customers and those who have made repeat purchases.\\n4. **Segmented Communication**: Develop separate marketing strategies for each category, tailoring messages and offers to meet the unique needs and preferences of each segment.\\n\\nBy implementing these recommendations, the client/user can improve customer engagement, increase sales, and drive business growth.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is the data is all about',\n",
       " 'context': [Document(metadata={'source': 'data_results.txt', 'row': 3}, page_content='summary:: A summary of the dataset\\nNone: including its source,context of data collection,the period covered,and the main variables included. This overview provides the structure and content of the data.'),\n",
       "  Document(metadata={'source': 'data_results.txt', 'row': 34}, page_content='summary:: Aggregated data summaries\\nNone: including key metrics and dimensions used for analyzing patterns and trends. Facts and cubes provide a multi-dimensional view of the data,facilitating in-depth analysis and insights.'),\n",
       "  Document(metadata={'source': 'data_results.txt', 'row': 30}, page_content='summary:: Data Facts and Cubes:')],\n",
       " 'answer': 'Based on the provided context, it appears that the dataset is likely related to some form of business or market analysis. The mention of \"facts and cubes\" and \"multi-dimensional view\" suggests that the data is being analyzed from various angles to gain insights into patterns and trends.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input':'what is the data is all about'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analytic_summary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object summary.report at 0x0000020B2E0DEB00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analytic_summary import SummaryGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object SummaryGenerator.generate at 0x000001B1533CA8C0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_generator = SummaryGenerator()\n",
    "summary_generator.load_data()\n",
    "summary_generator.chains()\n",
    "summary_generator.build_graph()\n",
    "summary_generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
